# LLM Cost & Latency Optimizer

A production-oriented **GenAI system design project** that dynamically optimizes **LLM cost, latency, and quality** using **policy-driven decision making**, **deterministic rules**, and **full explainability**.

This project demonstrates how real-world LLM systems are built beyond simple â€œprompt â†’ model â†’ responseâ€ pipelines.

---

## ğŸš€ Project Motivation

In most GenAI applications:

- The **same LLM** is used for every query
- The **same context size** is always sent
- The **same retrieval depth** is used

This leads to:
- Unnecessary **cost**
- Higher **latency**
- Poor **resource utilization**

### This project solves that.

It dynamically decides:
- Which **model tier** to use (SMALL / MEDIUM / LARGE)
- How much **context** to send
- How many documents to **retrieve**

Based on:
- Query complexity
- Optional **budget constraint**
- Optional **SLA (speed requirement)**

All decisions are:
- **Deterministic**
- **Explainable**
- **Traceable**
- **Testable**

---

## ğŸ§  Core Design Principles

1. **LLMs are used only for judgment, not decisions**
   - Classification and reasoning â†’ LLM
   - Enforcement and control â†’ deterministic code

2. **Policy-driven architecture**
   - Cost policy
   - Latency policy
   - Conflict resolution rules

3. **Full explainability**
   - Every step produces a structured JSON trace
   - Trace is returned to the UI

4. **Deterministic execution**
   - No randomness in core logic
   - Temperature = 0
   - Same input â†’ same output

5. **Async-first**
   - Parallel policy evaluation
   - Non-blocking LLM calls
   - Optimized latency

---

## ğŸ—ï¸ High-Level Architecture
User Query
â†“
Query Complexity Agent (LLM)
â†“
Cost Policy Agent â”€â”€â”
â”œâ”€â”€â–º Execution Controller (Deterministic Rules)
Latency Policy Agent â”˜
â†“
Optimized RAG Pipeline
â†“
Answer + Cost/Latency Estimates + Full Trace
---

## ğŸ”§ Key Concepts Used

- **LangChain (LCEL)** for composable chains
- **LangGraph** for explicit state orchestration
- **FAISS** for vector similarity search
- **Groq (LLaMA models)** for fast LLM inference
- **Streamlit** for interactive UI
- **Pydantic** for schema validation
- **tiktoken** for token counting

---

## ğŸ“ Project Structure
llm-cost-latency-optimizer/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ main.py                # LangGraph orchestration + CLI
â”œâ”€â”€ config.py              # Policies, tiers, defaults, conflict rules
â”œâ”€â”€ agents.py              # LLM-based policy agents (JSON outputs)
â”œâ”€â”€ core.py                # Deterministic controller + RAG logic
â”œâ”€â”€ utils.py               # Helpers (LLM calls, tracing, token counting)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
---

## ğŸ¤– Model Tiers

Instead of hardcoding models, the system uses **tiers**:

| Tier | Characteristics |
|----|----|
| SMALL | Low cost, low latency |
| MEDIUM | Balanced |
| LARGE | Higher cost, better reasoning |

Actual models are mapped via a registry, making the system **model-agnostic**.

---

## ğŸ“Š Decision Factors

### 1. Query Complexity
LLM-based classification:
- `simple`
- `medium`
- `complex`

Used only for **judgment**, not control.

---

### 2. Cost Policy
Based on:
- Complexity
- Optional budget

Produces:
- Allowed model tiers
- Maximum token budget

---

### 3. Latency Policy
Based on:
- Complexity
- SLA (`fast`, `balanced`, `relaxed`)

Produces:
- Preferred tier
- Maximum retrieval depth

---

### 4. Conflict Resolution (Deterministic)
Rules resolve conflicts such as:
- SLA vs Budget
- Context limits
- Retrieval depth limits

**Example rule:**
> SLA takes precedence over budget for latency-sensitive queries.

---

## ğŸ“š RAG Pipeline

- Documents embedded using HuggingFace embeddings
- Indexed using FAISS
- Retrieval depth dynamically controlled
- Context size capped based on policies
- Token-safe prompt construction

---

## ğŸ” Explainability & Tracing

Every step produces structured trace data:

json
{
  "step": "latency_policy",
  "inputs": {"sla": "fast"},
  "outputs": {"preferred_tier": "SMALL"},
  "timestamp": "..."
}

âš™ï¸ Setup Instructions
1. Clone repository
   git clone <repo-url>
   cd llm-cost-latency-optimizer
2. Install dependencies
   pip install -r requirements.txt
3. Environment setup
   cp .env.example .env
Add your Groq API key:
GROQ_API_KEY=your_key_here

---

â–¶ï¸ Run the Project
CLI Mode : python main.py --query "Explain transformers" --sla fast
Streamlit UI : streamlit run app.py

---

ğŸ¯ What This Project Demonstrates
	â€¢	Real-world GenAI system design
	â€¢	Policy-based LLM optimization
	â€¢	LangGraph orchestration
	â€¢	Deterministic decision making
	â€¢	Explainability-first architecture

This project is intentionally designed to reflect how production GenAI systems are built, not just demo-level applications.

---

ğŸ“Œ Author Notes

Built as a learning-focused project to deeply understand:
	â€¢	LLM cost/latency trade-offs
	â€¢	Async GenAI pipelines
	â€¢	Explainable AI system design
